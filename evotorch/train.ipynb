{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n",
    "To begin with, we import all necessary tools from EvoTorch, and also PyTorch so that we can jit save the learned policy for submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evotorch import Problem\n",
    "from evotorch.algorithms import PGPE\n",
    "from evotorch.neuroevolution import GymNE\n",
    "from evotorch.logging import StdOutLogger, PandasLogger\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration\n",
    "Now we configure for the specific environment we want to solve. We've figured this configuration out for the weighted reward keys through some trial and error, but we imagine that they can be modified further. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"myosuite:myoChallengeBaodingP1-v1\"\n",
    "policy_path = 'agent/policies/learned_policy_boading.pkl'\n",
    "env_config = {\n",
    "    'weighted_reward_keys' : {\n",
    "        'pos_dist_1':1.0,\n",
    "        'pos_dist_2':1.0,\n",
    "        'solved': 2,\n",
    "        'act_reg': 0.1,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have the configuration of the optimizer. Here we are generally following the setup used in previous work with ClipUp:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIPUP_MAX_SPEED = 0.15\n",
    "CLIPUP_ALPHA = CLIPUP_MAX_SPEED * 0.75\n",
    "RADIUS = CLIPUP_MAX_SPEED * 15\n",
    "\n",
    "STDEV_LR = 0.1\n",
    "STDEV_MAX_CHANGE = 0.2\n",
    "\n",
    "POPSIZE = 20000\n",
    "POPSIZE_MAX = POPSIZE * 8\n",
    "NUM_INTERACTIONS = int(POPSIZE * 200 * 0.75)\n",
    "NUM_GENERATIONS = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "Now we're ready to create the problem class. You should note that we're using the `Policy` class from the included `policy.py` file: our initial experiments suggest that a slightly more complex (and recurrent) policy is beneficial for these complex environments. Feel free to modify `Policy` as you try to improve upon this baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from policy import Policy\n",
    "\n",
    "problem = GymNE(\n",
    "    env_name=env_name,\n",
    "    env_config = env_config,\n",
    "    network=Policy,\n",
    "    network_args = {\n",
    "        'hidden_dim': 64,\n",
    "    },\n",
    "    observation_normalization=True,\n",
    "    num_actors='max',\n",
    ")\n",
    "print('Solution length is', problem.solution_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `problem` instantiated, we can create the searcher:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "searcher = PGPE(\n",
    "    problem,\n",
    "    center_learning_rate=CLIPUP_ALPHA,\n",
    "    optimizer=\"clipup\",\n",
    "    optimizer_config={\"max_speed\": CLIPUP_MAX_SPEED},\n",
    "    radius_init=RADIUS,\n",
    "    stdev_learning_rate=STDEV_LR,\n",
    "    stdev_max_change=STDEV_MAX_CHANGE,\n",
    "    popsize=POPSIZE,\n",
    "    popsize_max=POPSIZE_MAX,\n",
    "    num_interactions=NUM_INTERACTIONS,\n",
    "    distributed = True,\n",
    ")\n",
    "searcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add an additional `after_step_hook` which will save the policy after every generation. This means that you can asynchonously evaluate and submit agents, even as the evolutionary run continues!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def save_policy():\n",
    "    global problem, searcher, policy_path\n",
    "    policy = problem.to_policy(searcher.status['center'])\n",
    "    scripted_module = torch.jit.script(policy)\n",
    "    torch.jit.save(scripted_module, policy_path)\n",
    "\n",
    "searcher.after_step_hook.append(save_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For sanity, we're adding a StdOutLogger instance and a PandasLogger instance so that we can track what's going on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = StdOutLogger(searcher)\n",
    "pandas_logger = PandasLogger(searcher)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train\n",
    "And that's all there is to it! We're ready to train. As we go, we'll be creating `learned_policy_boading.pkl` in the local directory. Check out `README.md` to see how you can submit these learned agents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "searcher.run(NUM_GENERATIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, it's nice to see the mean evaluation of the training as time passes. You can interrupt the above cell at any time, generate the plot below, and restart the above cell to continue training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_logger.to_dataframe().mean_eval.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myosuite-challenge",
   "language": "python",
   "name": "myosuite-challenge"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
