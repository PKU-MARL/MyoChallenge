{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c87fa41",
   "metadata": {},
   "source": [
    "# Visualization + Testing\n",
    "This notebook can be used to visualize and test policies learned using train.ipynb. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611c5a92",
   "metadata": {},
   "source": [
    "## Setup\n",
    "First, we specify which environment we are using and change the expected path of the saved policy associated with that environment. We'll specify the env config later as we will change it to study different aspects of the behaviour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2189a73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"myosuite:myoChallengeBaodingP1-v1\"\n",
    "policy_path = 'agent/policies/learned_policy_boading.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b8351c",
   "metadata": {},
   "source": [
    "Next we'll load the policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11054b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "policy = torch.jit.load(policy_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681dccd8",
   "metadata": {},
   "source": [
    "## Testing success\n",
    "First, we will test the success rate of the policy. Even though P1 is supposedly deterministic, we have observed some small variation in outcome here, so we'll average over 10 runs to get a mean. First we instantiate the environment so that the reward returns 1 only when a successful step is made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7137435b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import myosuite\n",
    "import gym\n",
    "\n",
    "env_config = {\n",
    "        'weighted_reward_keys' : {\n",
    "            'pos_dist_1':.0,\n",
    "            'pos_dist_2':.0,\n",
    "            'solved':1.0,\n",
    "        },\n",
    "}\n",
    "    \n",
    "    \n",
    "env = gym.make(\n",
    "    env_name,\n",
    "    **env_config,\n",
    ")\n",
    "env.sim.render(mode='window')\n",
    "\n",
    "obs_space = env.observation_space\n",
    "act_space = env.action_space\n",
    "print('Environment has observation space', obs_space, 'action space', act_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a91a748",
   "metadata": {},
   "source": [
    "Now we can carry out 10 experiment repeats to approximate the success rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2b05b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evotorch.neuroevolution.net.layers import reset_module_state\n",
    "from evotorch.neuroevolution.net.rl import reset_env\n",
    "import numpy as np\n",
    "# List to track success rates\n",
    "success_rates = []\n",
    "# 10 episodes\n",
    "for _ in range(10):\n",
    "    # Reset the environment and policy\n",
    "    obs = reset_env(env)\n",
    "    policy = torch.jit.load(policy_path)\n",
    "    # Reset the observed number of successes\n",
    "    n_successes = 0.\n",
    "    length = 0\n",
    "    \n",
    "    done = False\n",
    "    # Run episode to termination\n",
    "    while not done:\n",
    "        # Get next action\n",
    "        with torch.no_grad():\n",
    "            act = policy(torch.as_tensor(obs, dtype=torch.float32, device=\"cpu\")).numpy()\n",
    "        # Apply action to environment\n",
    "        obs, re, done, _, = env.step(act)\n",
    "        # Render the environment\n",
    "        env.sim.render(mode='window')\n",
    "        n_successes += re #+ 1\n",
    "        length += 1\n",
    "    print('Observed', n_successes, 'successes corresponding to success rate', n_successes/200)\n",
    "    print('Episode length', length)\n",
    "    success_rates.append(n_successes / 200)\n",
    "env.close()\n",
    "\n",
    "print('Mean success rate', np.mean(success_rates))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b2adf1",
   "metadata": {},
   "source": [
    "## Testing effort\n",
    "Secondly, we will test the average effort usage of the policy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc4a5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import myosuite\n",
    "import gym\n",
    "\n",
    "env_config = {\n",
    "        'weighted_reward_keys' : {\n",
    "            'pos_dist_1':.0,\n",
    "            'pos_dist_2':.0,\n",
    "            'act_reg': -1.,\n",
    "        },\n",
    "}\n",
    "    \n",
    "env = gym.make(\n",
    "    env_name,\n",
    "    **env_config,\n",
    ")\n",
    "env.sim.render(mode='window')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6c9f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evotorch.neuroevolution.net.layers import reset_module_state\n",
    "from evotorch.neuroevolution.net.rl import reset_env\n",
    "import numpy as np\n",
    "\n",
    "# List to track effort\n",
    "effort = []\n",
    "# 10 episodes\n",
    "for _ in range(10):\n",
    "    # Reset the environment and policy\n",
    "    obs = reset_env(env)\n",
    "    policy = torch.jit.load(policy_path)\n",
    "    # Reset the observed total_effort\n",
    "    total_effort = 0.\n",
    "    n_steps = 0\n",
    "    \n",
    "    done = False\n",
    "    # Run episode to termination\n",
    "    while not done:\n",
    "        # Get next action\n",
    "        with torch.no_grad():\n",
    "            act = policy(torch.as_tensor(obs, dtype=torch.float32, device=\"cpu\")).numpy()\n",
    "        # Apply action to environment\n",
    "        obs, re, done, _, = env.step(act)\n",
    "        # Render the environment\n",
    "        env.sim.render(mode='window')\n",
    "        total_effort += re\n",
    "        n_steps += 1\n",
    "    print('Observed', total_effort, 'over', n_steps, 'steps giving average effort', total_effort/n_steps)\n",
    "    effort.append(total_effort/n_steps)\n",
    "env.close()\n",
    "\n",
    "print('Mean effort', np.mean(effort))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927a61fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myosuite-challenge",
   "language": "python",
   "name": "myosuite-challenge"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
